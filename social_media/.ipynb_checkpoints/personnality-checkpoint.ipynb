{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0b5ec7a-c4fa-47d5-a970-9d12011cab11",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Data management\n",
    "import pandas as pd\n",
    "\n",
    "# Models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pickle\n",
    "# Model Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from text_processing import tokenize_and_lemmatize\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from myFunctions import *\n",
    "import warnings\n",
    "import time\n",
    "start= time.time()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbacd1e1-6d14-48ba-8fd7-708720aa5e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement buildins\n",
      "ERROR: No matching distribution found for buildins\n"
     ]
    }
   ],
   "source": [
    "# !pip install buildins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "967837f4-d0c0-4df1-9e4d-a5aa5c1bf05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df=pd.read_csv('../media/essays.csv', encoding='cp1252')\n",
    "# Drop IS column \n",
    "df = df.drop(['#AUTHID'], axis=1)\n",
    "# Rename columns\n",
    "df.columns = ['posts', 'extraversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness']\n",
    "# Main Loop to go through all columns and create models\n",
    "cols = ['extraversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eef88568-ce96-4b50-bb1d-14d42ba158e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def undersample_majority(df, cls_col):\n",
    "    \"\"\" Function to equalize minority and majority classes\"\"\"\n",
    "\n",
    "    counter = Counter(df[cls_col])\n",
    "    ratio = min(counter.values()) / max(counter.values())\n",
    "    #inv = (max(counter.values()) / min(counter.values()))\n",
    "    #print(f'Minority Class: Majority Class Ratio = 1:{inv:.2f}')\n",
    "    # Convert columns to arrays\n",
    "    if ratio < 0.7:\n",
    "        X = np.array(df['posts']).reshape(-1, 1)\n",
    "        y = np.array(df[cls_col])\n",
    "        # Undersample majority class to the minority class size\n",
    "        under = RandomUnderSampler()\n",
    "        X, y = under.fit_resample(X, y)\n",
    "        print(Counter(y))\n",
    "        df = pd.DataFrame({'posts': list(X.flatten()), cls_col: y}, columns=['posts', cls_col])\n",
    "    else:\n",
    "        df = df\n",
    "    return df\n",
    "\n",
    "# Function to remove links and symbols\n",
    "def clear_text(data):\n",
    "\n",
    "    cleaned_text=[]\n",
    "    for sentence in data.posts:\n",
    "        sentence=sentence.lower()\n",
    "\n",
    "#       removing links from text data\n",
    "        sentence=re.sub(r'http[s]?://\\S+', '',sentence)\n",
    "\n",
    "#       removing other symbols\n",
    "        sentence=re.sub(r'@([a-zA-Z0-9_]{1,50})', '', sentence)\n",
    "        sentence=re.sub(r'#([a-zA-Z0-9_]{1,50})', '', sentence)\n",
    "        sentence=re.sub(r'[^A-Za-z]+', ' ', sentence)\n",
    "        sentence=\" \".join([word for word in sentence.split() if not len(word) < 3])\n",
    "\n",
    "        cleaned_text.append(sentence)\n",
    "    return cleaned_text\n",
    "\n",
    "# Function to remove contractions\n",
    "def fix_contractions(df, column_name = \"posts\"):\n",
    "    df[column_name] = df[column_name].apply(lambda x: contractions.fix(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb53c846-4638-4f00-9e84-6aa33e16b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lemmatization class\n",
    "# class Lemmatizer(object):\n",
    "#     def __init__(self):\n",
    "#         self.lemmatizer = WordNetLemmatizer()\n",
    "#     def __call__(self, sentence):\n",
    "#         return [self.lemmatizer.lemmatize(word) for word in sentence.split() if len(word)>2]\n",
    "# def tokenize_and_lemmatize(text):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     tokens = text.split()\n",
    "#     lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "#     return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d0f54-b2b9-46bb-89ee-dd39795117f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b23c8dae-5418-4c98-83e0-e470209dc295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mInitializing training of extraversion data\u001b[0m\n",
      "\u001b[1mResampling imbalanced data\u001b[0m\n",
      "Cleaning extraversion training data...\n",
      "Cleaning extraversion testing data...\n",
      "Preparing classifiers...\n",
      "Running logistic regression...\n",
      "Running Linear Support Vector classifier...\n",
      "Running Support Vector classifier...\n",
      "Running Multinomial Naive Bayes...\n",
      "Running Decision Tree classifier...\n",
      "Running Random Forest classifier...\n",
      "Running XGBoost classifier...\n",
      "Running kNN classifier...\n",
      "\u001b[93mAnalyzing..\u001b[0m\n",
      "Saving Support Vector classifier\n",
      "Saving logistic regression\n",
      "Saving Random Forest classifier\n",
      "Saving Multinomial Naive Bayes\n",
      "Saving Linear Support Vector classifier\n",
      "Best models to predict extraversion are ['Support Vector classifier', 'logistic regression', 'Random Forest classifier', 'Multinomial Naive Bayes', 'Linear Support Vector classifier'] with a mean accuracy of 55.83%\n",
      "\u001b[35mInitializing training of neuroticism data\u001b[0m\n",
      "\u001b[1mResampling imbalanced data\u001b[0m\n",
      "Cleaning neuroticism training data...\n",
      "Cleaning neuroticism testing data...\n",
      "Preparing classifiers...\n",
      "Running logistic regression...\n",
      "Running Linear Support Vector classifier...\n",
      "Running Support Vector classifier...\n",
      "Running Multinomial Naive Bayes...\n",
      "Running Decision Tree classifier...\n",
      "Running Random Forest classifier...\n",
      "Running XGBoost classifier...\n",
      "Running kNN classifier...\n",
      "\u001b[93mAnalyzing..\u001b[0m\n",
      "Saving Random Forest classifier\n",
      "Saving Multinomial Naive Bayes\n",
      "Saving XGBoost classifier\n",
      "Saving Linear Support Vector classifier\n",
      "Saving Support Vector classifier\n",
      "Best models to predict neuroticism are ['Random Forest classifier', 'Multinomial Naive Bayes', 'XGBoost classifier', 'Linear Support Vector classifier', 'Support Vector classifier'] with a mean accuracy of 57.98%\n",
      "\u001b[35mInitializing training of agreeableness data\u001b[0m\n",
      "\u001b[1mResampling imbalanced data\u001b[0m\n",
      "Cleaning agreeableness training data...\n",
      "Cleaning agreeableness testing data...\n",
      "Preparing classifiers...\n",
      "Running logistic regression...\n",
      "Running Linear Support Vector classifier...\n",
      "Running Support Vector classifier...\n",
      "Running Multinomial Naive Bayes...\n",
      "Running Decision Tree classifier...\n",
      "Running Random Forest classifier...\n",
      "Running XGBoost classifier...\n",
      "Running kNN classifier...\n",
      "\u001b[93mAnalyzing..\u001b[0m\n",
      "Saving Multinomial Naive Bayes\n",
      "Saving Support Vector classifier\n",
      "Saving kNN Classifier\n",
      "Saving Linear Support Vector classifier\n",
      "Saving Decision Tree classifier\n",
      "Best models to predict agreeableness are ['Multinomial Naive Bayes', 'Support Vector classifier', 'kNN Classifier', 'Linear Support Vector classifier', 'Decision Tree classifier'] with a mean accuracy of 52.83%\n",
      "\u001b[35mInitializing training of conscientiousness data\u001b[0m\n",
      "\u001b[1mResampling imbalanced data\u001b[0m\n",
      "Cleaning conscientiousness training data...\n",
      "Cleaning conscientiousness testing data...\n",
      "Preparing classifiers...\n",
      "Running logistic regression...\n",
      "Running Linear Support Vector classifier...\n",
      "Running Support Vector classifier...\n",
      "Running Multinomial Naive Bayes...\n",
      "Running Decision Tree classifier...\n",
      "Running Random Forest classifier...\n",
      "Running XGBoost classifier...\n",
      "Running kNN classifier...\n",
      "\u001b[93mAnalyzing..\u001b[0m\n",
      "Saving Multinomial Naive Bayes\n",
      "Saving Support Vector classifier\n",
      "Saving logistic regression\n",
      "Saving XGBoost classifier\n",
      "Saving Random Forest classifier\n",
      "Best models to predict conscientiousness are ['Multinomial Naive Bayes', 'Support Vector classifier', 'logistic regression', 'XGBoost classifier', 'Random Forest classifier'] with a mean accuracy of 57.25%\n",
      "\u001b[35mInitializing training of openness data\u001b[0m\n",
      "\u001b[1mResampling imbalanced data\u001b[0m\n",
      "Cleaning openness training data...\n",
      "Cleaning openness testing data...\n",
      "Preparing classifiers...\n",
      "Running logistic regression...\n",
      "Running Linear Support Vector classifier...\n",
      "Running Support Vector classifier...\n",
      "Running Multinomial Naive Bayes...\n",
      "Running Decision Tree classifier...\n",
      "Running Random Forest classifier...\n",
      "Running XGBoost classifier...\n",
      "Running kNN classifier...\n",
      "\u001b[93mAnalyzing..\u001b[0m\n",
      "Saving Multinomial Naive Bayes\n",
      "Saving Support Vector classifier\n",
      "Saving logistic regression\n",
      "Saving Random Forest classifier\n",
      "Saving XGBoost classifier\n",
      "Best models to predict openness are ['Multinomial Naive Bayes', 'Support Vector classifier', 'logistic regression', 'Random Forest classifier', 'XGBoost classifier'] with a mean accuracy of 63.24%\n",
      "\u001b[102mTraining completed successfully in 3.68 Minutes\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for name in cols:\n",
    "\n",
    "    data = df[[name, 'posts']].copy()\n",
    "#    Split the data\n",
    "#   Use stratify split to ensure equal distribution of data\n",
    "    print('\\033[35m' +  'Initializing training of '+ name + ' data' + '\\033[0m')\n",
    "\n",
    "    train_data,test_data=train_test_split(data,test_size=0.2,random_state=42,stratify=data[name])\n",
    "    print('\\033[1m' + 'Resampling imbalanced data' + '\\033[0m')\n",
    "#   Adjust imbalanced data \n",
    "    train_data = undersample_majority(train_data, name)\n",
    "#   Remove contractions\n",
    "    train_data = fix_contractions(train_data)\n",
    "    test_data = fix_contractions(test_data)\n",
    "#   Clean the data\n",
    "    print(f'Cleaning {name} training data...')\n",
    "    train_data.posts =clear_text(train_data)\n",
    "    print(f'Cleaning {name} testing data...')\n",
    "    test_data.posts =clear_text(test_data)\n",
    "    print('Preparing classifiers...')\n",
    "#   Vectorize the text\n",
    "    vectorizer=TfidfVectorizer(max_features=5000,stop_words='english',tokenizer=tokenize_and_lemmatize)\n",
    "    vectorizer.fit(train_data.posts)\n",
    "    train_post=vectorizer.transform(train_data.posts).toarray()\n",
    "    test_post=vectorizer.transform(test_data.posts).toarray()\n",
    "#   Encode the labels\n",
    "    target_encoder=LabelEncoder()\n",
    "    train_target=target_encoder.fit_transform(train_data[name])\n",
    "    test_target=target_encoder.transform(test_data[name])\n",
    "#   Declare an empty dictionary to store model name and accuracy\n",
    "    models_accuracy={}\n",
    "#   Declare an empty list to store trained models\n",
    "    models = []\n",
    "#   Create classification object\n",
    "    print(f'Running logistic regression...')\n",
    "    model_log=LogisticRegression()\n",
    "#   Train the model using the training set\n",
    "    model_log.fit(train_post,train_target)\n",
    "#   Add model accuracy of the testing set to the dictionary\n",
    "    models_accuracy['logistic regression']=accuracy_score(test_target,model_log.predict(test_post))\n",
    "#   Add the trained model to the list\n",
    "    models.append(model_log)\n",
    "    print(f'Running Linear Support Vector classifier...')\n",
    "    model_linear_svc=LinearSVC()\n",
    "    model_linear_svc.fit(train_post,train_target)\n",
    "    models_accuracy['Linear Support Vector classifier']=accuracy_score(test_target,model_linear_svc.predict(test_post))\n",
    "    models.append(model_linear_svc)\n",
    "    print(f'Running Support Vector classifier...')\n",
    "    model_svc=SVC()\n",
    "    model_svc.fit(train_post,train_target)\n",
    "    models_accuracy['Support Vector classifier']=accuracy_score(test_target,model_svc.predict(test_post))\n",
    "    models.append(model_svc)\n",
    "    print(f'Running Multinomial Naive Bayes...')\n",
    "    model_multinomial_nb=MultinomialNB()\n",
    "    model_multinomial_nb.fit(train_post,train_target)\n",
    "    models_accuracy['Multinomial Naive Bayes']=accuracy_score(test_target,model_multinomial_nb.predict(test_post))\n",
    "    models.append(model_multinomial_nb)\n",
    "    print(f'Running Decision Tree classifier...')\n",
    "    model_tree=DecisionTreeClassifier()\n",
    "    model_tree.fit(train_post,train_target)\n",
    "    models_accuracy['Decision Tree classifier']=accuracy_score(test_target,model_tree.predict(test_post))\n",
    "    models.append(model_tree)\n",
    "    print(f'Running Random Forest classifier...')\n",
    "    model_forest=RandomForestClassifier()\n",
    "    model_forest.fit(train_post,train_target)\n",
    "    models_accuracy['Random Forest classifier']=accuracy_score(test_target,model_forest.predict(test_post))\n",
    "    models.append(model_forest)\n",
    "    print(f'Running XGBoost classifier...')\n",
    "    model_xgb=XGBClassifier(gpu_id=-1)\n",
    "    model_xgb.fit(train_post,train_target)\n",
    "    models_accuracy['XGBoost classifier']=accuracy_score(test_target,model_xgb.predict(test_post))\n",
    "    models.append(model_xgb)\n",
    "    print(f'Running kNN classifier...')\n",
    "    model_knn=KNeighborsClassifier()\n",
    "    model_knn.fit(train_post,train_target)\n",
    "    models_accuracy['kNN Classifier']=accuracy_score(test_target,model_knn.predict(test_post))\n",
    "    models.append(model_knn)\n",
    "    print('\\033[93m' + 'Analyzing..' + '\\033[0m')\n",
    "#   Convert the dictionary to a dataframe\n",
    "    accuracy=pd.DataFrame(models_accuracy.items(),columns=['Classifier','Accuracy'])\n",
    "#   Append the models to the dataframe\n",
    "    accuracy['Model'] = models\n",
    "#   Re-order the dataframe based on model accuracy\n",
    "    accuracy = accuracy.sort_values(by='Accuracy',ascending=False,ignore_index=True)\n",
    "#   Save the top 5 models\n",
    "    for i in range(5):\n",
    "        pickle.dump(accuracy.Model[i], open('../media/'+name+'_model'+str(i)+'.sav', 'wb'))\n",
    "        print(f'Saving {accuracy.Classifier[i]}')\n",
    "#   Save vectorizer and encoder\n",
    "    pickle.dump(vectorizer, open('../media/'+name+'_vectors.pickle', 'wb'))\n",
    "    pickle.dump(target_encoder, open('../media/'+name+'_encoder.obj', 'wb'))\n",
    "    # Add names of top 5 models to list\n",
    "    top = list(accuracy.Classifier.head())\n",
    "    acc = list(accuracy.Accuracy.head())\n",
    "    avg = sum(acc)/len(acc)\n",
    "    print(f'Best models to predict {name} are {top} with a mean accuracy of {avg:.2%}')\n",
    "#   Timing runtime\n",
    "tt = (((time.time()-start)/60))\n",
    "print('\\033[102m' +  f'Training completed successfully in {tt:.2f} Minutes' + '\\033[0m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8c58878-d3d3-4c2b-ab4c-922ca76023a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('../media/mbti_1.csv')\n",
    "\n",
    "#Function to split types in MBTI dataset\n",
    "def divide_mbti_types(df):\n",
    "    df[\"EI\"] = df['type'].astype(str).str[0]\n",
    "    df[\"NS\"] = df['type'].astype(str).str[1]\n",
    "    df[\"FT\"] = df['type'].astype(str).str[2]\n",
    "    df[\"JP\"] = df['type'].astype(str).str[3]\n",
    "    return df\n",
    "\n",
    "# Divide columns\n",
    "df = divide_mbti_types(df)\n",
    "\n",
    "# Main Loop to go through all columns and create models\n",
    "cols = ['EI', 'NS', 'FT', 'JP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf0f7348-4e5c-4136-b826-f37b16acd841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mInitializing training of EI data\u001b[0m\n",
      "\u001b[1mResampling imbalanced data\u001b[0m\n",
      "Counter({'E': 1599, 'I': 1599})\n",
      "Cleaning EI training data...\n",
      "Cleaning EI testing data...\n",
      "Preparing classifiers...\n",
      "Running logistic regression...\n",
      "Running Linear Support Vector classifier...\n",
      "Running Support Vector classifier...\n",
      "Running Multinomial Naive Bayes...\n",
      "Running Decision Tree classifier...\n",
      "Running Random Forest classifier...\n",
      "Running XGBoost classifier...\n",
      "Running kNN classifier...\n",
      "\u001b[93mAnalyzing..\u001b[0m\n",
      "Saving logistic regression\n",
      "Saving Support Vector classifier\n",
      "Saving XGBoost classifier\n",
      "Saving Linear Support Vector classifier\n",
      "Saving Multinomial Naive Bayes\n",
      "Best models to predict EI are ['logistic regression', 'Support Vector classifier', 'XGBoost classifier', 'Linear Support Vector classifier', 'Multinomial Naive Bayes'] with a mean accuracy of 80.31%\n",
      "\u001b[35mInitializing training of NS data\u001b[0m\n",
      "\u001b[1mResampling imbalanced data\u001b[0m\n",
      "Counter({'N': 958, 'S': 958})\n",
      "Cleaning NS training data...\n",
      "Cleaning NS testing data...\n",
      "Preparing classifiers...\n",
      "Running logistic regression...\n",
      "Running Linear Support Vector classifier...\n",
      "Running Support Vector classifier...\n",
      "Running Multinomial Naive Bayes...\n",
      "Running Decision Tree classifier...\n",
      "Running Random Forest classifier...\n",
      "Running XGBoost classifier...\n",
      "Running kNN classifier...\n",
      "\u001b[93mAnalyzing..\u001b[0m\n",
      "Saving logistic regression\n",
      "Saving Support Vector classifier\n",
      "Saving Linear Support Vector classifier\n",
      "Saving XGBoost classifier\n",
      "Saving Random Forest classifier\n",
      "Best models to predict NS are ['logistic regression', 'Support Vector classifier', 'Linear Support Vector classifier', 'XGBoost classifier', 'Random Forest classifier'] with a mean accuracy of 81.81%\n",
      "\u001b[35mInitializing training of FT data\u001b[0m\n",
      "\u001b[1mResampling imbalanced data\u001b[0m\n",
      "Cleaning FT training data...\n",
      "Cleaning FT testing data...\n",
      "Preparing classifiers...\n",
      "Running logistic regression...\n",
      "Running Linear Support Vector classifier...\n",
      "Running Support Vector classifier...\n",
      "Running Multinomial Naive Bayes...\n",
      "Running Decision Tree classifier...\n",
      "Running Random Forest classifier...\n",
      "Running XGBoost classifier...\n",
      "Running kNN classifier...\n",
      "\u001b[93mAnalyzing..\u001b[0m\n",
      "Saving logistic regression\n",
      "Saving Support Vector classifier\n",
      "Saving XGBoost classifier\n",
      "Saving Linear Support Vector classifier\n",
      "Saving Multinomial Naive Bayes\n",
      "Best models to predict FT are ['logistic regression', 'Support Vector classifier', 'XGBoost classifier', 'Linear Support Vector classifier', 'Multinomial Naive Bayes'] with a mean accuracy of 83.45%\n",
      "\u001b[35mInitializing training of JP data\u001b[0m\n",
      "\u001b[1mResampling imbalanced data\u001b[0m\n",
      "Counter({'J': 2747, 'P': 2747})\n",
      "Cleaning JP training data...\n",
      "Cleaning JP testing data...\n",
      "Preparing classifiers...\n",
      "Running logistic regression...\n",
      "Running Linear Support Vector classifier...\n",
      "Running Support Vector classifier...\n",
      "Running Multinomial Naive Bayes...\n",
      "Running Decision Tree classifier...\n",
      "Running Random Forest classifier...\n",
      "Running XGBoost classifier...\n",
      "Running kNN classifier...\n",
      "\u001b[93mAnalyzing..\u001b[0m\n",
      "Saving logistic regression\n",
      "Saving Support Vector classifier\n",
      "Saving XGBoost classifier\n",
      "Saving Linear Support Vector classifier\n",
      "Saving Random Forest classifier\n",
      "Best models to predict JP are ['logistic regression', 'Support Vector classifier', 'XGBoost classifier', 'Linear Support Vector classifier', 'Random Forest classifier'] with a mean accuracy of 76.90%\n",
      "\u001b[102mTraining completed successfully in 16.06 Minutes\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for name in cols:\n",
    "\n",
    "    data = df[[name, 'posts']].copy()\n",
    "#    Split the data\n",
    "#   Use stratify split to ensure equal distribution of data\n",
    "    print('\\033[35m' + 'Initializing training of ' + name + ' data' + '\\033[0m')\n",
    "\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data[name])\n",
    "    print('\\033[1m' + 'Resampling imbalanced data' + '\\033[0m')\n",
    "#   Adjust imbalanced data \n",
    "    train_data = undersample_majority(train_data, name)\n",
    "#   Remove contractions\n",
    "    train_data = fix_contractions(train_data)\n",
    "    test_data = fix_contractions(test_data)\n",
    "#   Clean the data\n",
    "    print(f'Cleaning {name} training data...')\n",
    "    train_data.posts = clear_text(train_data)\n",
    "    print(f'Cleaning {name} testing data...')\n",
    "    test_data.posts = clear_text(test_data)\n",
    "    print('Preparing classifiers...')\n",
    "#   Vectorize the text\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', tokenizer=tokenize_and_lemmatize)\n",
    "    vectorizer.fit(train_data.posts)\n",
    "    train_post = vectorizer.transform(train_data.posts).toarray()\n",
    "    test_post = vectorizer.transform(test_data.posts).toarray()\n",
    "#   Encode the labels\n",
    "    target_encoder=LabelEncoder()\n",
    "    train_target=target_encoder.fit_transform(train_data[name])\n",
    "    test_target=target_encoder.transform(test_data[name])\n",
    "#   Declare an empty dictionary to store model name and accuracy\n",
    "    models_accuracy={}\n",
    "#   Declare an empty list to store trained models\n",
    "    models = []\n",
    "#   Create classification object\n",
    "    print(f'Running logistic regression...')\n",
    "    model_log=LogisticRegression()\n",
    "#   Train the model using the training set\n",
    "    model_log.fit(train_post,train_target)\n",
    "#   Add model accuracy of the testing set to the dictionary\n",
    "    models_accuracy['logistic regression']=accuracy_score(test_target,model_log.predict(test_post))\n",
    "#   Add the trained model to the list\n",
    "    models.append(model_log)\n",
    "    print(f'Running Linear Support Vector classifier...')\n",
    "    model_linear_svc=LinearSVC()\n",
    "    model_linear_svc.fit(train_post,train_target)\n",
    "    models_accuracy['Linear Support Vector classifier']=accuracy_score(test_target,model_linear_svc.predict(test_post))\n",
    "    models.append(model_linear_svc)\n",
    "    print(f'Running Support Vector classifier...')\n",
    "    model_svc=SVC()\n",
    "    model_svc.fit(train_post,train_target)\n",
    "    models_accuracy['Support Vector classifier']=accuracy_score(test_target,model_svc.predict(test_post))\n",
    "    models.append(model_svc)\n",
    "    print(f'Running Multinomial Naive Bayes...')\n",
    "    model_multinomial_nb=MultinomialNB()\n",
    "    model_multinomial_nb.fit(train_post,train_target)\n",
    "    models_accuracy['Multinomial Naive Bayes']=accuracy_score(test_target,model_multinomial_nb.predict(test_post))\n",
    "    models.append(model_multinomial_nb)\n",
    "    print(f'Running Decision Tree classifier...')\n",
    "    model_tree=DecisionTreeClassifier()\n",
    "    model_tree.fit(train_post,train_target)\n",
    "    models_accuracy['Decision Tree classifier']=accuracy_score(test_target,model_tree.predict(test_post))\n",
    "    models.append(model_tree)\n",
    "    print(f'Running Random Forest classifier...')\n",
    "    model_forest=RandomForestClassifier()\n",
    "    model_forest.fit(train_post,train_target)\n",
    "    models_accuracy['Random Forest classifier']=accuracy_score(test_target,model_forest.predict(test_post))\n",
    "    models.append(model_forest)\n",
    "    print(f'Running XGBoost classifier...')\n",
    "    model_xgb=XGBClassifier(gpu_id=-1)\n",
    "    model_xgb.fit(train_post,train_target)\n",
    "    models_accuracy['XGBoost classifier']=accuracy_score(test_target,model_xgb.predict(test_post))\n",
    "    models.append(model_xgb)\n",
    "    print(f'Running kNN classifier...')\n",
    "    model_knn=KNeighborsClassifier()\n",
    "    model_knn.fit(train_post,train_target)\n",
    "    models_accuracy['kNN Classifier']=accuracy_score(test_target,model_knn.predict(test_post))\n",
    "    models.append(model_knn)\n",
    "    print('\\033[93m' + 'Analyzing..' + '\\033[0m')\n",
    "#   Convert the dictionary to a dataframe\n",
    "    accuracy=pd.DataFrame(models_accuracy.items(),columns=['Classifier','Accuracy'])\n",
    "#   Append the models to the dataframe\n",
    "    accuracy['Model'] = models\n",
    "#   Re-order the dataframe based on model accuracy\n",
    "    accuracy = accuracy.sort_values(by='Accuracy',ascending=False,ignore_index=True)\n",
    "#   Save the top 5 models\n",
    "    for i in range(5):\n",
    "        pickle.dump(accuracy.Model[i], open('../media/'+name+'_model'+str(i)+'.sav', 'wb'))\n",
    "        print(f'Saving {accuracy.Classifier[i]}')\n",
    "#   Save vectorizer and encoder\n",
    "    pickle.dump(vectorizer, open('../media/'+name+'_vectors.pickle', 'wb'))\n",
    "    pickle.dump(target_encoder, open('../media/'+name+'_encoder.obj', 'wb'))\n",
    "    # Add names of top 5 models to list\n",
    "    top = list(accuracy.Classifier.head())\n",
    "    acc = list(accuracy.Accuracy.head())\n",
    "    avg = sum(acc)/len(acc)\n",
    "    print(f'Best models to predict {name} are {top} with a mean accuracy of {avg:.2%}')\n",
    "#   Timing runtime\n",
    "tt = (((time.time()-start)/60))\n",
    "print('\\033[102m' +  f'Training completed successfully in {tt:.2f} Minutes' + '\\033[0m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f538fe8c-c665-4016-a5cc-08b213e7390f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
